__author__ = "Dylan Warnecke"
__email__ = "dylan.warnecke@gmail.com"

import numpy as np
import pandas as pd
import pickle
from pathlib import Path


class HiddenMarkovModel:
    """
    Hidden Markov Model for regime detection. Most of this code is generated by Copilot.
    :param states: Number of hidden states.
    :param num_features: Number of features in the data.
    """

    def __init__(self, states: int, features: int) -> None:
        """
        Initialize the Hidden Markov Model with random parameters.
        :param states: Number of hidden states.
        :param features: Number of features in the data.
        """
        self.num_states = states
        self.num_features = features
        self.pi = self._init_probabilities(states)
        self.A = self._init_probabilities((states, states))
        self.means = self._init_means(states, features)
        self.covs = self._init_covariances(states, features)

    def train(self, data: pd.DataFrame, max_iter: int = 50, tol: float = 1e-4) -> None:
        """
        Train the hidden Markov model using the Baum-Welch algorithm.
        :param data: Input data for training the HMM.
        :param max_iter: Maximum number of EM iterations.
        :param tol: Convergence tolerance for log-likelihood improvement.
        """
        prev_loglik = None
        for iteration in range(max_iter):
            logB, log_alpha, log_scale, log_beta = self._e_step(data)
            gamma, xi = self._compute_gamma_xi(logB, log_alpha, log_beta)
            self._m_step(data, gamma, xi)
            loglik = self._compute_log_likelihood(log_scale)
            if prev_loglik is not None and abs(loglik - prev_loglik) < tol:
                break
            prev_loglik = loglik

    def calc_state_proba(self, data: pd.DataFrame) -> np.ndarray:
        """
        Calculate state probabilities for all timesteps in the observation data.
        :param data: DataFrame of normalized features.
        :returns: Array of shape (T, n_states) with state probabilities for each timestep
        """
        logB = self._compute_log_emission_probs(data)
        log_alpha, log_scale = self._log_forward(logB)
        log_beta = self._log_backward(logB, log_scale)
        log_gamma = log_alpha + log_beta
        log_gamma -= log_gamma.max(axis=1, keepdims=True)  # Numerical stability
        gamma = np.exp(log_gamma)
        gamma = gamma / gamma.sum(axis=1, keepdims=True)
        return gamma

    def save(self, filepath: str | Path) -> None:
        """
        Save the trained HMM model parameters to a file.
        :param filepath: Path where to save the model.
        """
        filepath = Path(filepath)
        filepath.parent.mkdir(parents=True, exist_ok=True)

        model_data = {
            "num_states": self.num_states,
            "num_features": self.num_features,
            "pi": self.pi,
            "A": self.A,
            "means": self.means,
            "covs": self.covs,
        }

        with open(filepath, "wb") as f:
            pickle.dump(model_data, f)

    @classmethod
    def load(cls, filepath: str | Path) -> "HiddenMarkovModel":
        """
        Create a new HMM instance by loading parameters from a file.
        :param filepath: Path to the saved model file.
        :returns: New HiddenMarkovModel instance with loaded parameters.
        """
        filepath = Path(filepath)
        with open(filepath, "rb") as f:
            model_data = pickle.load(f)

        # File contains all necessary parameters for the HMM
        hmm = cls(model_data["num_states"], model_data["num_features"])
        hmm.pi = model_data["pi"]
        hmm.A = model_data["A"]
        hmm.means = model_data["means"]
        hmm.covs = model_data["covs"]

        return hmm

    def _init_probabilities(self, shape: int | tuple[int, ...]) -> np.ndarray:
        """
        Initialize random probability vector or matrix and normalize.
        :param shape: int or tuple for shape.
        :returns: Normalized probability array.
        """
        arr = np.random.rand(*((shape,) if isinstance(shape, int) else shape))
        # Add small epsilon to avoid division by zero
        eps = 1e-8
        if isinstance(shape, int):
            arr = np.maximum(arr, eps)
            arr /= arr.sum()
        else:
            arr = np.maximum(arr, eps)
            arr /= arr.sum(axis=1, keepdims=True)
        return arr

    def _init_means(self, states: int, features: int) -> np.ndarray:
        """
        Initialize means for Gaussian emissions.
        :param states: Number of states.
        :param features: Number of features.
        :returns: Means array.
        """
        means = np.zeros((states, features))
        for i in range(features):
            means[:, i] = np.random.uniform(-1, 1, size=states)
        return means

    def _init_covariances(self, states: int, features: int) -> np.ndarray:
        """
        Initialize covariance matrices for Gaussian emissions.
        :param states: Number of states.
        :param features: Number of features.
        :returns: Covariance matrices.
        """
        covs = np.array([np.eye(features) for _ in range(states)])
        # Add small regularization to diagonal for numerical stability
        covs += np.eye(features) * 1e-6
        return covs

    def _log_gaussian_prob(
        self, x: np.ndarray, mean: np.ndarray, cov: np.ndarray
    ) -> float:
        """
        Compute the log-probability of x under a multivariate normal distribution.
        :param x: Observation vector.
        :param mean: Mean vector.
        :param cov: Covariance matrix.
        :returns: Log-probability density value.
        """
        size = len(x)
        cov_reg = cov + np.eye(size) * 1e-3  # Stronger regularization
        # Check for NaN/inf in cov_reg
        if not np.all(np.isfinite(cov_reg)):
            cov_reg = np.eye(size) * 1e-2
        # Ensure covariance is positive definite
        try:
            sign, logdet = np.linalg.slogdet(cov_reg)
            if sign <= 0 or not np.isfinite(logdet):
                cov_reg = np.eye(size) * 1e-2
                sign, logdet = np.linalg.slogdet(cov_reg)
        except Exception:
            cov_reg = np.eye(size) * 1e-2
            sign, logdet = np.linalg.slogdet(cov_reg)
        x_mu = x - mean
        try:
            inv_cov = np.linalg.inv(cov_reg)
        except Exception:
            inv_cov = np.eye(size) * 1e2
        quad_form = x_mu @ inv_cov @ x_mu.T
        if not np.isfinite(quad_form):
            quad_form = 0.0
        return -0.5 * (size * np.log(2 * np.pi) + logdet + quad_form)

    def _compute_log_emission_probs(self, data: pd.DataFrame) -> np.ndarray:
        """
        Compute log-emission probabilities for all states and all time steps.
        :param data: DataFrame of observations.
        :returns: Log-emission probability matrix (T x N).
        """
        T = data.shape[0]
        N = self.num_states
        logB = np.zeros((T, N))
        for t in range(T):
            x = np.asarray(data.iloc[t].values)
            for j in range(N):
                logB[t, j] = self._log_gaussian_prob(x, self.means[j], self.covs[j])
        return logB

    def _log_forward(self, logB: np.ndarray) -> tuple[np.ndarray, np.ndarray]:
        """
        Forward algorithm for HMM in log-space.
        :param logB: Log-emission probability matrix (T x N).
        :returns: Log-alpha matrix (T x N), log-scaling factors (T,).
        """
        T, N = logB.shape
        log_alpha = np.zeros((T, N))
        log_scale = np.zeros(T)
        log_pi = np.log(self.pi + 1e-16)
        log_alpha[0] = log_pi + logB[0]
        log_scale[0] = np.logaddexp.reduce(log_alpha[0])
        if not np.isfinite(log_scale[0]):
            log_scale[0] = 0.0
        log_alpha[0] -= log_scale[0]
        logA = np.log(self.A + 1e-16)
        for t in range(1, T):
            for j in range(N):
                val = np.logaddexp.reduce(log_alpha[t - 1] + logA[:, j])
                if not np.isfinite(val):
                    val = 0.0
                log_alpha[t, j] = logB[t, j] + val
            log_scale[t] = np.logaddexp.reduce(log_alpha[t])
            if not np.isfinite(log_scale[t]):
                log_scale[t] = 0.0
            log_alpha[t] -= log_scale[t]
        return log_alpha, log_scale

    def _log_backward(self, logB: np.ndarray, log_scale: np.ndarray) -> np.ndarray:
        """
        Backward algorithm for HMM in log-space.
        :param logB: Log-emission probability matrix (T x N).
        :param log_scale: Log-scaling factors from forward pass.
        :returns: Log-beta matrix (T x N).
        """
        T, N = logB.shape
        log_beta = np.zeros((T, N))
        log_beta[-1] = -log_scale[-1] if np.isfinite(log_scale[-1]) else 0.0
        logA = np.log(self.A + 1e-16)
        for t in reversed(range(T - 1)):
            for i in range(N):
                val = np.logaddexp.reduce(logA[i] + logB[t + 1] + log_beta[t + 1])
                if not np.isfinite(val):
                    val = 0.0
                log_beta[t, i] = val
            log_beta[t] -= log_scale[t] if np.isfinite(log_scale[t]) else 0.0
        return log_beta

    def _e_step(
        self, data: pd.DataFrame
    ) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """
        Perform the E-step: compute log-emission, log-forward, and log-backward probabilities.
        :param data: Input data.
        :returns: (logB, log_alpha, log_scale, log_beta)
        """
        logB = self._compute_log_emission_probs(data)
        log_alpha, log_scale = self._log_forward(logB)
        log_beta = self._log_backward(logB, log_scale)
        return logB, log_alpha, log_scale, log_beta

    def _compute_gamma_xi(
        self, logB: np.ndarray, log_alpha: np.ndarray, log_beta: np.ndarray
    ) -> tuple[np.ndarray, np.ndarray]:
        """
        Compute gamma and xi for the Baum-Welch algorithm in log-space.
        :param logB: Log-emission probabilities.
        :param log_alpha: Log-forward probabilities.
        :param log_beta: Log-backward probabilities.
        :returns: (gamma, xi)
        """
        T, N = logB.shape
        logA = np.log(self.A + 1e-16)
        log_gamma = log_alpha + log_beta
        norm = np.logaddexp.reduce(log_gamma, axis=1, keepdims=True)
        norm = np.where(np.isfinite(norm), norm, 0.0)
        log_gamma -= norm
        gamma = np.exp(log_gamma)
        gamma = np.where(np.isfinite(gamma), gamma, 0.0)
        xi = np.zeros((T - 1, N, N))
        for t in range(T - 1):
            log_xi_t = log_alpha[t][:, None] + logA + logB[t + 1] + log_beta[t + 1]
            norm_xi = np.logaddexp.reduce(log_xi_t.ravel())
            norm_xi = norm_xi if np.isfinite(norm_xi) else 0.0
            log_xi_t -= norm_xi
            xi[t] = np.exp(log_xi_t)
            xi[t] = np.where(np.isfinite(xi[t]), xi[t], 0.0)
        return gamma, xi

    def _m_step(self, data: pd.DataFrame, gamma: np.ndarray, xi: np.ndarray) -> None:
        """
        Perform the M-step: update model parameters.
        :param data: Input data.
        :param gamma: State occupancy probabilities.
        :param xi: State transition probabilities.
        """
        N = self.num_states
        self.pi = gamma[0]
        self.pi = np.maximum(self.pi, 1e-8)
        self.pi /= self.pi.sum()
        self.A = xi.sum(axis=0)
        self.A = np.maximum(self.A, 1e-8)
        self.A /= self.A.sum(axis=1, keepdims=True)
        for j in range(N):
            weights = gamma[:, j]
            weights = np.maximum(weights, 1e-8)
            weighted_sum = np.sum(weights[:, None] * data.values, axis=0)
            denom = weights.sum()
            if denom == 0 or not np.isfinite(denom):
                denom = 1.0
            self.means[j] = weighted_sum / denom
            diff = data.values - self.means[j]
            cov = (weights[:, None, None] * (diff[:, :, None] @ diff[:, None, :])).sum(
                axis=0
            ) / denom
            # Regularize covariance (stronger)
            cov += np.eye(data.shape[1]) * 1e-3
            # Check for NaN/inf or non-positive-definite
            if not np.all(np.isfinite(cov)) or np.linalg.det(cov) <= 0:
                cov = np.eye(data.shape[1]) * 1e-2
            self.covs[j] = cov

    def _compute_log_likelihood(self, log_scale: np.ndarray) -> float:
        """
        Compute the log-likelihood of the observed data given the current model.
        :param log_scale: Log-scaling factors from the forward algorithm.
        :returns: Log-likelihood value.
        """
        return np.sum(log_scale)
